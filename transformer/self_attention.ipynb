{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ec3535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_pseudo_embedding(w):\n",
    "    return np.random.rand(64)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def self_attention(embeddings):\n",
    "    q = np.array(embeddings)\n",
    "    k = np.array(embeddings)\n",
    "    v = np.array(embeddings)\n",
    "\n",
    "    scores = np.dot(q, k.T)\n",
    "    attention_weights = softmax(scores)\n",
    "    weighted_sum = np.dot(attention_weights, v)\n",
    "    return attention_weights, weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "701d368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog\"    \n",
    "words = sentence.split()\n",
    "embeddings = [generate_pseudo_embedding(w) for w in words]\n",
    "attention_weights, weighted_sum = self_attention(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52c9c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: The quick brown fox jumps over the lazy dog\n",
      "For the word: The\n",
      "Attention for: The: 0.928\n",
      "Attention for: over: 0.016\n",
      "Attention for: quick: 0.012\n",
      "For the word: quick\n",
      "Attention for: quick: 0.488\n",
      "Attention for: lazy: 0.004\n",
      "Attention for: over: 0.003\n",
      "For the word: brown\n",
      "Attention for: brown: 0.818\n",
      "Attention for: fox: 0.018\n",
      "Attention for: lazy: 0.013\n",
      "For the word: fox\n",
      "Attention for: fox: 0.854\n",
      "Attention for: quick: 0.011\n",
      "Attention for: lazy: 0.003\n",
      "For the word: jumps\n",
      "Attention for: jumps: 0.95\n",
      "Attention for: quick: 0.078\n",
      "Attention for: lazy: 0.077\n",
      "For the word: over\n",
      "Attention for: over: 0.653\n",
      "Attention for: quick: 0.008\n",
      "Attention for: dog: 0.003\n",
      "For the word: the\n",
      "Attention for: the: 0.996\n",
      "Attention for: quick: 0.314\n",
      "Attention for: lazy: 0.276\n",
      "For the word: lazy\n",
      "Attention for: lazy: 0.607\n",
      "Attention for: quick: 0.034\n",
      "Attention for: dog: 0.006\n",
      "For the word: dog\n",
      "Attention for: dog: 0.763\n",
      "Attention for: quick: 0.043\n",
      "Attention for: over: 0.019\n"
     ]
    }
   ],
   "source": [
    "print(\"Original sentence:\", sentence)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    print(\"For the word:\", word)\n",
    "    top_3 = sorted(range(len(words)),\n",
    "                   key=lambda j: attention_weights[i][j],\n",
    "                   reverse=True)[:3]\n",
    "    for j in top_3:\n",
    "        print(f\"Attention for: {words[j]}: {np.round(attention_weights[i][j], 3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base-13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
